{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ahmad\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_gnn as tfg\n",
    "\n",
    "import functools\n",
    "import collections\n",
    "\n",
    "from typing import Mapping,List\n",
    "import math\n",
    "import googlemaps\n",
    "import pandas as pd\n",
    "import torch\n",
    "from MTGNN.net import gtnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hotspot ID                                    2\n",
       "Location      Universidad Complutense de Madrid\n",
       "lat                                   40.445437\n",
       "lon                                   -3.729942\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize Google Maps client with your API key\n",
    "gmaps = googlemaps.Client(key='AIzaSyDL0Foh_O74N0xUxHeKw5AwiVjiPLs-Hjw')\n",
    "\n",
    "# List of locations in Madrid\n",
    "madrid_locations = [\n",
    "    'Santiago Bernabéu Stadium',\n",
    "    'Universidad Complutense de Madrid',\n",
    "    'Adolfo Suárez Madrid–Barajas Airport',\n",
    "    'European University of Madrid (Alcobendas Campus)',\n",
    "    'Universidad Politécnica de Madrid',\n",
    "    'ESCP Business School - Madrid campus',\n",
    "    'Cerralbo Museum',\n",
    "    'Royal Palace of Madrid',\n",
    "    'Puerta de Alcalá',\n",
    "    'San Bernardo',\n",
    "    'Las Ventas Bullring'\n",
    "]\n",
    "\n",
    "# Fetch latitude and longitude for each location\n",
    "location_data = []\n",
    "for i, location in enumerate(madrid_locations):\n",
    "    geocode_result = gmaps.geocode(location + ', Madrid, Spain')\n",
    "    if geocode_result:\n",
    "        lat = geocode_result[0]['geometry']['location']['lat']\n",
    "        lon = geocode_result[0]['geometry']['location']['lng']\n",
    "        location_data.append({\n",
    "            'Hotspot ID': i + 1,  # Assign a unique Hotspot ID\n",
    "            'Location': location,\n",
    "            'Latitude': lat,\n",
    "            'Longitude': lon\n",
    "        })\n",
    "    else:\n",
    "        location_data.append({\n",
    "            'Hotspot ID': i + 1,\n",
    "            'Location': location,\n",
    "            'Latitude': None,\n",
    "            'Longitude': None\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "madrid_df = pd.DataFrame(location_data)\n",
    "\n",
    "madrid_df.rename(columns={'Latitude':'lat', 'Longitude':'lon'}, inplace=True)\n",
    "madrid_df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\ahmad\\Downloads\\worldaware-raw-data.csv\", parse_dates=['timestamp'])\n",
    "df\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.weekday\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "df.set_index('timestamp', inplace=True)  # Set 'timestamp' as the index\n",
    "\n",
    "cols = ['region_id','ad_id','ad_type','is_click']\n",
    "df = df.drop(cols, axis=1)\n",
    "df\n",
    "\n",
    "# NUMBER OF UNIQUE IDS\n",
    "unique_ids_count = df['device_id'].nunique()\n",
    "\n",
    "# TIME STAMPS PER USER\n",
    "timestamps_by_user = df.groupby('device_id').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['lat', 'lon']] = scaler.fit_transform(df[['lat', 'lon']])\n",
    "madrid_df[['lat', 'lon']] = scaler.transform(madrid_df[['lat', 'lon']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.26649478  30.19570716 339.35146552 ...  63.7740705   24.80402446\n",
      "  117.78544494]\n",
      " [ 62.82554944  30.59191647 338.91429331 ...  63.40558242  24.49144376\n",
      "  117.37122717]\n",
      " [ 55.24868785  37.67337185 331.37869673 ...  57.03432108  19.59460092\n",
      "  110.1816466 ]\n",
      " ...\n",
      " [377.21855936 293.6681393  644.28455707 ... 364.54268794 331.18808548\n",
      "  420.85696652]\n",
      " [376.42793422 292.9157815  643.46222903 ... 363.73435588 330.39511762\n",
      "  420.04298236]\n",
      " [376.43697129 292.92533049 643.47047496 ... 363.74302556 330.40411307\n",
      "  420.05150132]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from math import radians\n",
    "\n",
    "\n",
    "df[['lat', 'lon']] = np.radians(df[['lat', 'lon']])\n",
    "madrid_df[['lat', 'lon']] = np.radians(madrid_df[['lat', 'lon']])\n",
    "\n",
    "# Prepare coordinates\n",
    "coords_users = df[['lat', 'lon']].to_numpy()\n",
    "coords_pois = madrid_df[['lat', 'lon']].to_numpy()\n",
    "\n",
    "# Calculate distances (multiply by Earth’s radius to convert to kilometers)\n",
    "distances = haversine_distances(coords_users, coords_pois) * 6371\n",
    "print(distances)\n",
    "\n",
    "threshold = 0.9 # kilometers\n",
    "adjacency_matrix = (distances < threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_id      12136.000000\n",
       "duration          15.000000\n",
       "lat                0.002847\n",
       "lon               -0.009219\n",
       "hour_of_day        0.000000\n",
       "day_of_week        5.000000\n",
       "Name: 2024-04-13 00:06:43, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gtnet(\n",
       "  (filter_convs): ModuleList(\n",
       "    (0-2): 3 x dilated_inception(\n",
       "      (tconv): ModuleList(\n",
       "        (0): Conv2d(32, 8, kernel_size=(1, 2), stride=(1, 1))\n",
       "        (1): Conv2d(32, 8, kernel_size=(1, 3), stride=(1, 1))\n",
       "        (2): Conv2d(32, 8, kernel_size=(1, 6), stride=(1, 1))\n",
       "        (3): Conv2d(32, 8, kernel_size=(1, 7), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gate_convs): ModuleList(\n",
       "    (0-2): 3 x dilated_inception(\n",
       "      (tconv): ModuleList(\n",
       "        (0): Conv2d(32, 8, kernel_size=(1, 2), stride=(1, 1))\n",
       "        (1): Conv2d(32, 8, kernel_size=(1, 3), stride=(1, 1))\n",
       "        (2): Conv2d(32, 8, kernel_size=(1, 6), stride=(1, 1))\n",
       "        (3): Conv2d(32, 8, kernel_size=(1, 7), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (residual_convs): ModuleList(\n",
       "    (0-2): 3 x Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (skip_convs): ModuleList(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 13), stride=(1, 1))\n",
       "    (1): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1))\n",
       "    (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (gconv1): ModuleList(\n",
       "    (0-2): 3 x mixprop(\n",
       "      (nconv): nconv()\n",
       "      (mlp): linear(\n",
       "        (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gconv2): ModuleList(\n",
       "    (0-2): 3 x mixprop(\n",
       "      (nconv): nconv()\n",
       "      (mlp): linear(\n",
       "        (mlp): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): ModuleList(\n",
       "    (0): LayerNorm((32, 114060, 13), eps=1e-05, elementwise_affine=True)\n",
       "    (1): LayerNorm((32, 114060, 7), eps=1e-05, elementwise_affine=True)\n",
       "    (2): LayerNorm((32, 114060, 1), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (start_conv): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (gc): graph_constructor(\n",
       "    (emb1): Embedding(114060, 40)\n",
       "    (emb2): Embedding(114060, 40)\n",
       "    (lin1): Linear(in_features=40, out_features=40, bias=True)\n",
       "    (lin2): Linear(in_features=40, out_features=40, bias=True)\n",
       "  )\n",
       "  (end_conv_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (end_conv_2): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (skip0): Conv2d(4, 64, kernel_size=(1, 19), stride=(1, 1))\n",
       "  (skipE): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_nodes = len(madrid_df) + len(df)  # Total number of nodes\n",
    "\n",
    "# Convert adjacency matrix to tensor and ensure it's on the correct device\n",
    "predefined_A = torch.tensor(adjacency_matrix, dtype=torch.float, device=device)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize MTGNN model\n",
    "model = gtnet(\n",
    "    gcn_true=True,\n",
    "    buildA_true=True,\n",
    "    gcn_depth=2,\n",
    "    num_nodes=num_nodes,\n",
    "    predefined_A=predefined_A,\n",
    "    device=device,  # Pass the device argument here\n",
    "    dropout=0.3,\n",
    "    subgraph_size=20,\n",
    "    node_dim=40,\n",
    "    dilation_exponential=1,\n",
    "    conv_channels=32,\n",
    "    residual_channels=32,\n",
    "    skip_channels=64,\n",
    "    end_channels=128,\n",
    "    seq_length=11,  # length of your input sequence\n",
    "    in_dim=4,       # number of input features\n",
    "    out_dim=2,      # number of output predictions (lat and lon)\n",
    "    layers=3\n",
    ")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "combined_df = pd.concat([df[['lat', 'lon']], madrid_df[['lat', 'lon']]], ignore_index=True)\n",
    "\n",
    "# Converting DataFrame to tensor\n",
    "features = torch.tensor(combined_df.values, dtype=torch.float).to(device)\n",
    "# Example targets as a placeholder. Adjust this based on your actual targets.\n",
    "targets = torch.randn(len(features), 2).to(device)  # Dummy targets\n",
    "\n",
    "\n",
    "train_features, temp_features, train_targets, temp_targets = train_test_split(\n",
    "    features, targets, test_size=0.3, random_state=42)\n",
    "val_features, test_features, val_targets, test_targets = train_test_split(\n",
    "    temp_features, temp_targets, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "batch_size = 32  # Adjust this as necessary\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_targets)\n",
    "val_dataset = TensorDataset(val_features, val_targets)\n",
    "test_dataset = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
